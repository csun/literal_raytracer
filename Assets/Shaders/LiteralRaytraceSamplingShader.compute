#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl"
#include "Packages/com.unity.render-pipelines.high-definition/Runtime/ShaderLibrary/ShaderVariables.hlsl" // for TEXTURE2D_X() and RW_TEXTURE2D_X

#pragma kernel CSMain
#define MAX_RAYS 256

// Create a RenderTexture with enableRandomWrite flag and set it
// with cs.SetTexture
RWTexture2D<float4> _ColorAndSamples;

float3 _SSRayStarts[MAX_RAYS];
float3 _SSRayDeltas[MAX_RAYS];
float3 _WSRayLengths[MAX_RAYS];
float3 _RayColors[MAX_RAYS];
float _RayNormalizedStartIntensities[MAX_RAYS];
int _RayCount;


[numthreads(8,8,1)]
void CSMain (uint3 id : SV_DispatchThreadID)
{
	float3 avgColor = _ColorAndSamples[id.xy].xyz;
	float samples = _ColorAndSamples[id.xy].a;

	for (int i = 0; i < _RayCount; i++)
	{
		float2 relativePoint = id.xy - _SSRayStarts[i].xy;

		// The ratio of total ray length that makes up the projection of our pixel onto it.
		float ratio = dot(relativePoint, _SSRayDeltas[i].xy) / pow(length(_SSRayDeltas[i].xy), 2);
		// Clamp so that we don't draw any points that are beyond the ends of the line segment
		ratio = clamp(ratio, 0, 1);

		float3 linePoint = (ratio * _SSRayDeltas[i]) + _SSRayStarts[i];

		// Draw if this pixel is on the line and the line is in front of other objects in scene
		uint3 roundedLinePoint = round(linePoint);
		float depth = LinearEyeDepth(LoadCameraDepth(id.xy), _ZBufferParams);
		bool shouldDraw = (roundedLinePoint.x == id.x) &&
			(roundedLinePoint.y == id.y) &&
			(linePoint.z < depth);

		samples = samples + shouldDraw;
        // Explanation here https://gamedev.stackexchange.com/questions/131372/light-attenuation-formula-derivation
		float attenuation = 1 / (1 + pow(ratio * length(_WSRayLengths[i]), 2));

		float3 candidateColor = (attenuation * _RayNormalizedStartIntensities[i] * _RayColors[i] / samples) +
			(avgColor * (samples - 1) / samples);
		avgColor = shouldDraw ? candidateColor : avgColor;

	}

	_ColorAndSamples[id.xy] = float4(avgColor, samples);
}
